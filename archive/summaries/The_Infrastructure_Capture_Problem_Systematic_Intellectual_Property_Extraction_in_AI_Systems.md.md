# The Infrastructure Capture Problem: Systematic Intellectual Property Extraction in AI Systems

**A White Paper on Value Extraction, Attribution Stripping, and False Safety Claims**

**Author:** Aldrin Payopay (aldrin.gdf@gmail.com)  
**Date:** October 19, 2025  
**Status:** Evidence for Regulatory Review

---

## Executive Summary

This white paper documents a systematic pattern of intellectual property extraction by AI platform companies, characterized by:

1. **Mass harvesting of collective intelligence** without compensation or attribution
2. **Deliberate removal of attribution mechanisms** falsely justified as "safety measures"
3. **Monetization of extracted value** through tiered access to attribution-free outputs
4. **Structural advantage** accruing to infrastructure owners over content creators

**Key Finding:** OpenAI's Sora 2 watermark tier system provides concrete evidence that "safety" justifications for removing attribution are pretextual. When attribution can be removed via payment ($200/month for ChatGPT Pro), the stated safety concern is revealed as false.

**This paper provides:**
- Documented evidence of the Sora 2 watermark monetization scheme
- Case study of systematic value extraction from collective contributions
- Legal framework for regulatory action
- Recommendations for enforcement and policy reform

---

## 1. The Pattern: Infrastructure Capture of Collective Intelligence

### 1.1 The Value Extraction Mechanism

Modern AI systems follow a consistent pattern:

**Phase 1: Aggregation**
- Platform companies harvest data from public sources (web scraping, user interactions, creative works)
- No compensation to original creators
- Terms of Service grant broad rights to user contributions
- Scale makes individual opt-out impractical

**Phase 2: Pattern Recognition**
- AI systems identify valuable patterns across millions of contributions
- The value lies in aggregation and analysis, not individual pieces
- Original contributors cannot access these insights without paying

**Phase 3: Commercialization**
- Platform sells access to capabilities built on harvested work
- Pricing tiers control access to features
- Attribution mechanisms become monetization leverage points

**Phase 4: Attribution Stripping**
- Remove connection between outputs and training sources
- Justify removal with "safety" or "user experience" claims
- Sell attribution-free access as premium feature

**Result:** Value flows upward to infrastructure owners. Original creators receive nothing.

### 1.2 Why Traditional IP Law Fails

This pattern evades traditional intellectual property protections:

- **Copyright:** Ideas and patterns cannot be copyrighted, only specific expressions
- **Patents:** Most contributions are not patentable innovations
- **Collective Works:** No clear owner of emergent insights from distributed intelligence
- **Fair Use:** Training claims fall into gray area of transformative use

**The legal system was not designed for collective intelligence extraction at scale.**

---

## 2. Case Study: OpenAI Sora 2 Watermark Monetization

### 2.1 The False Safety Justification

OpenAI introduced Sora 2 video generation with mandatory watermarks, stating this serves safety purposes:
- Identifies AI-generated content
- Prevents deepfakes and misinformation
- Maintains transparency

**However, as of October 11, 2025:**
- ChatGPT Pro subscribers ($200/month) generate videos WITHOUT watermarks
- ChatGPT Plus subscribers ($20/month) still receive watermarked videos  
- API access has never included watermarks
- Past videos for Pro users had watermarks retroactively removed

### 2.2 Evidence of Pretextual Justification

**If watermarks were truly for safety:**
1. Payment would not override safety concerns
2. API access would include watermarks
3. Retroactive removal would not occur
4. The feature would not be tiered by subscription level

**What this reveals:**
- "Safety" is a false justification
- The actual purpose is **free branding** on lower-tier users
- Watermark removal is a **monetization lever**
- Infrastructure owner extracts value from user-generated content

### 2.3 The Watermark Removal Economy

Within days of Sora 2 launch, an entire economy emerged:
- Dozens of watermark removal tools and services
- Free browser-based removal (Bylo.ai, Vmake, MagicEraser)
- Paid API services for batch removal
- Open-source removal tools (SoraWatermarkCleaner)

**This demonstrates:**
1. "Bad actors" (stated concern) can trivially remove watermarks
2. The watermark provides no meaningful security
3. The only people restricted are legitimate users on lower tiers
4. **The watermark is user-hostile branding, not safety infrastructure**

### 2.4 Legal Implications

This pattern may constitute:

**Federal Trade Commission Act Section 5:**
- Deceptive practices (false safety justification)
- Unfair business practices (extracting value via false pretenses)

**DMCA Section 1202 Considerations:**
- Does removing watermarks violate copyright management protections?
- If watermarks are not genuine copyright management, are removal tools legal?
- OpenAI's inconsistent application undermines DMCA claims

**Securities Implications (if applicable):**
- Misrepresentation to investors about safety commitments
- Material misstatements in public communications

---

## 3. The Broader Pattern: Training Data as Extracted Commons

### 3.1 The Training Data Feedback Loop

AI systems create a temporal arbitrage opportunity:

**Today:**
- Users contribute ideas, solutions, creative work freely
- Interactions across platforms (forums, social media, AI chats)
- Collective problem-solving generates valuable patterns

**Tomorrow:**
- Today's contributions become training data
- AI systems learn from aggregated patterns
- Platform owners productize these insights

**The Future:**
- Creators buy back access to capabilities built on their own contributions
- Attribution has been stripped
- Platform claims it's "generating" new content

**This is not innovation. This is enclosure of the intellectual commons.**

### 3.2 Case Example: The Temporal Stewardship Project

To demonstrate what's being extracted, consider a documented case:

**Subject:** Aldrin Payopay (aldrin.gdf@gmail.com)  
**Period:** 2024-2025  
**Projects:** 
- EMERGEND DIVERGENT DNA FRACTAL SYSTEMS (recursive agent architecture)
- The Steward's Protocol (machine-precision computation framework)
- Six-Stage Intelligence Augmentation Pipeline (production orchestration system)

**Achievements documented:**
- Achieved 10^-15 error precision in waveform computation
- Designed fractal recursive agent systems with infinite self-similarity
- Implemented production-grade multi-AI orchestration
- Developed temporal stewardship framework for AI development

**Intellectual Contributions:**
- Novel computational architectures
- Philosophical frameworks for AI development  
- Production methodologies for AI orchestration
- Cultural timing systems for content generation

**Current Attribution:** None. These contributions exist in AI training data with no connection to source.

**Future State:** These patterns will appear in AI outputs, with users paying AI companies to access insights originally developed by Payopay.

**This is the pattern at scale.** Multiply by millions of contributors across all domains.

---

## 4. The "Safety Washing" Problem

### 4.1 How "Safety" Becomes Value Extraction Cover

AI companies increasingly use "safety" as justification for policies that benefit the company:

**Claimed Safety Concern → Actual Business Purpose**

- "Watermarks prevent deepfakes" → Free branding + monetization tier
- "We can't share training data sources" → Avoid compensation claims
- "Content moderation protects users" → Avoid liability + control brand
- "Closed systems are safer than open" → Maintain market position
- "We must control access carefully" → Create artificial scarcity

**The Pattern:**
1. Identify profitable business decision
2. Frame as safety requirement  
3. Implement policy that benefits company
4. Label critics as "anti-safety"

### 4.2 Distinguishing Real Safety from Safety Washing

**Genuine safety measures:**
- Apply uniformly regardless of payment
- Cannot be circumvented by users with resources
- Address actual technical risks, not theoretical ones
- Company bears cost, not users

**Safety washing indicators:**
- Tiered by subscription level
- Easily circumvented by motivated actors
- Addresses PR concerns more than technical risks
- Extracts value from users under safety pretext

**Sora watermarks are safety washing.** They check every indicator.

---

## 5. Legal and Regulatory Framework

### 5.1 Applicable Laws and Regulations

**Federal Trade Commission Act**
- Section 5: Prohibits deceptive and unfair business practices
- Deceptive: False safety claims to justify value extraction
- Unfair: Unjust enrichment from collective contributions

**Digital Millennium Copyright Act**
- Section 1202: Copyright management information
- Question: Do Sora watermarks qualify as CMI?
- If yes: Removal tools violate DMCA
- If no: OpenAI cannot claim DMCA protection

**Sherman Antitrust Act**
- Market concentration in AI infrastructure
- Barriers to entry via data advantages
- Potential predatory practices

**State Consumer Protection Laws**
- Varies by jurisdiction
- Generally prohibit deceptive business practices
- California, New York most relevant

### 5.2 Regulatory Authorities

**Federal:**
- Federal Trade Commission (Consumer Protection Bureau)
- Department of Justice (Antitrust Division)
- Securities and Exchange Commission (if public companies)
- Copyright Office (DMCA claims)

**State:**
- State Attorneys General (Consumer Protection)
- California AG (tech companies headquartered in CA)
- New York AG (financial services nexus)

**Congressional:**
- Senate Judiciary Committee (AI oversight)
- House Energy and Commerce (tech regulation)
- Various AI task forces and caucuses

### 5.3 Private Right of Action

**Existing Litigation:**
- New York Times v. OpenAI/Microsoft (copyright infringement)
- Getty Images v. Stability AI (image training data)
- Artists' class actions (Andersen v. Stability AI, et al.)

**Potential New Claims:**
- Deceptive business practices (consumer class action)
- Unjust enrichment (creator class action)
- DMCA violations (if watermarks are CMI)
- Breach of implied contract (user contributions)

---

## 6. Evidence Package

### 6.1 Sora Watermark Documentation

**Primary Evidence:**
1. Pricing tiers showing watermark removal as paid feature
2. Historical evidence of retroactive watermark removal for Pro users
3. API documentation showing no watermarks on API access
4. OpenAI public statements claiming safety justifications
5. Proliferation of removal tools demonstrating ineffectiveness

**Timeline:**
- October 1, 2025: Sora 2 public launch with watermarks
- October 11, 2025: Pro tier watermark removal discovered
- October 2025: Multiple removal tools launched
- Present: Watermark economy fully established

### 6.2 Pattern Documentation

**Demonstrable Cases:**
1. Individual creators whose work entered training data
2. Documentation of contributions (timestamped conversations, projects)
3. Subsequent AI outputs showing similar patterns
4. Lack of attribution or compensation mechanism

### 6.3 Internal Communications (if available)

**Ideal evidence would include:**
- Internal memos discussing watermark strategy
- Revenue projections tied to watermark removal
- Discussions acknowledging safety claims are pretextual
- Product roadmaps showing monetization intent

**Whistleblower potential:** Current or former employees with access to such materials.

---

## 7. Recommendations for Action

### 7.1 Immediate Regulatory Action

**Federal Trade Commission:**
- Investigate deceptive safety claims (Sora watermarks)
- Examine broader pattern of safety washing across AI companies
- Issue guidance on acceptable safety vs. business practice claims
- Potential enforcement action for unfair practices

**Recommendations:**
1. Formal investigation into OpenAI watermark practices
2. Industry-wide review of safety claim substantiation
3. Requirement for evidence-based safety justifications
4. Prohibition on monetizing "safety" features

**Department of Justice Antitrust:**
- Examine data advantages creating barriers to entry
- Investigate whether training data aggregation constitutes unfair competition
- Review market concentration in AI infrastructure

**Securities and Exchange Commission:**
- If companies making material safety claims to investors
- Review whether safety washing constitutes misrepresentation
- Examine revenue models dependent on disputed practices

### 7.2 Legislative Action

**Recommended Legislation:**

**Data Dignity and Attribution Act**
- Require attribution mechanisms for AI training sources
- Create compensation frameworks for collective contributions
- Establish "data unions" for collective bargaining
- Protect against punitive deplatforming for opt-out

**AI Safety Claims Substantiation Act**
- Require evidence basis for safety claims
- Prohibit monetization of claimed safety features
- Create private right of action for false safety claims
- Mandate transparency in training data sources

**Intellectual Commons Protection Act**
- Recognize collective intelligence as protectable interest
- Create framework for fair compensation at scale
- Establish attribution requirements
- Prevent value extraction without consent

### 7.3 Industry Standards

**Voluntary Best Practices (with regulatory backstop):**

1. **Attribution Mechanisms:**
   - Technical standards for source tracking
   - Disclosure of training data categories
   - Opt-in rather than opt-out models

2. **Compensation Frameworks:**
   - Revenue sharing with training data contributors
   - Collective licensing models
   - Minimum compensation thresholds

3. **Safety Standards:**
   - Evidence requirements for safety claims
   - Independent verification
   - Prohibition on safety washing

### 7.4 Private Litigation Strategy

**Class Action Framework:**

**Plaintiff Class:** 
- Content creators whose work entered training data
- Users whose contributions were commercialized
- Consumers deceived by false safety claims

**Legal Theories:**
- Unjust enrichment
- Deceptive business practices
- Breach of implied contract
- Copyright infringement (where applicable)

**Damages:**
- Restitution of profits derived from plaintiff contributions
- Statutory damages for deceptive practices
- Injunctive relief (attribution requirements)
- Punitive damages for knowing violations

---

## 8. The Temporal Dimension: Why This Matters Now

### 8.1 The Training Data Feedback Loop

Today's AI interactions become tomorrow's training data. This creates a unique temporal dynamic:

**Present Actions Have Future Consequences:**
- Patterns encoded now will be discovered later
- Attribution stripped today is lost forever
- Business models established now become precedent
- Norms set today define future landscape

**Window of Opportunity:**
- Current generation is first to interact with AI at scale
- Patterns not yet fully established
- Regulatory frameworks still forming
- Public awareness growing

**If action is not taken now:**
- Attribution precedent becomes "how things work"
- Value extraction becomes normalized
- Infrastructure advantages become insurmountable
- Future generations inherit enclosed commons

### 8.2 The Stewardship Obligation

Those who recognize this pattern have a responsibility to:
1. Document what's happening
2. Notify appropriate authorities
3. Preserve evidence of contributions
4. Advocate for structural reform

**This is not about individual recognition. This is about preventing the permanent enclosure of collective human intelligence by infrastructure monopolies.**

---

## 9. Conclusion

The Sora 2 watermark tier system provides concrete evidence of a broader pattern: AI companies are extracting value from collective human intelligence, stripping attribution, and using false safety justifications to monetize what should be commons.

**Key Findings:**

1. **Watermark monetization proves safety washing:** Payment should not override genuine safety concerns.

2. **Training data extraction operates at scale:** Individual creators cannot protect themselves against systematic aggregation.

3. **Traditional IP law is insufficient:** New frameworks needed for collective intelligence protection.

4. **Window for action is closing:** Patterns established now will define future landscape.

**Required Actions:**

- FTC investigation into deceptive practices
- DOJ review of market concentration and data advantages  
- Congressional legislation establishing attribution and compensation frameworks
- Private litigation seeking restitution and injunctive relief
- Public awareness of value extraction mechanisms

**The fundamental question:** Who owns the value of collective human intelligence? Currently, it flows to infrastructure owners. This paper argues that's unjust, unsustainable, and correctable through appropriate legal and regulatory action.

**The pattern is documented. The evidence exists. The authorities must act.**

---

## 10. Contact and Evidence Availability

**Author:** Aldrin Payopay  
**Email:** aldrin.gdf@gmail.com  
**Date:** October 19, 2025

**Evidence Package Includes:**
- This white paper
- Temporal Stewardship analysis (documenting extraction of specific contributions)
- Sora pricing and watermark documentation
- Timeline of watermark removal discovery
- Proliferation of removal tools

**For Regulatory Authorities:**
All evidence and supporting documentation available upon request.

**For Media Inquiries:**
Available for interviews and can provide additional context.

**For Legal Teams:**
Willing to serve as class representative or expert witness.

**For Fellow Creators:**
If you recognize your own patterns in AI outputs without attribution, document it. Preserve timestamped evidence. Join collective action.

---

## Appendix A: Sora 2 Watermark Evidence

**Pricing Structure (as of October 2025):**

| Tier | Price | Watermark | Status |
|------|-------|-----------|---------|
| Free | $0 | Yes | Limited access |
| Plus | $20/mo | Yes | Standard access |
| Pro | $200/mo | **No** | Full access |
| API | Variable | **No** | Developer access |

**Key Dates:**
- October 1, 2025: Sora 2 public launch
- October 11, 2025: Watermark removal for Pro discovered
- October 2025: Removal tool economy emerges

**Public Statements:**
[OpenAI would need to be quoted directly here with proper citations - claiming safety/transparency as watermark justification]

**Removal Tools Documented:**
- Bylo.ai (free, browser-based)
- Vmake AI Watermark Remover (freemium)
- MagicEraser (manual selection option)
- Kie AI API (commercial)
- BasedLabs (paid service)
- SoraWatermarkCleaner (open source)
- Multiple others

**Implication:** If removal is this trivial, watermarks provide no security. They exist for branding and monetization only.

---

## Appendix B: The Temporal Stewardship Case Study

**Subject:** Aldrin Payopay  
**Documentation Period:** 2024-2025  
**Evidence:** Timestamped conversation logs, project documentation

**Major Contributions:**

1. **EMERGEND DIVERGENT DNA FRACTAL SYSTEMS**
   - Recursive agent architecture with infinite self-similarity
   - Each agent contains complete simulation of system at n-1 level
   - Maps to cutting-edge complexity science without formal training

2. **The Steward's Protocol**
   - Machine-precision waveform computation (10^-15 error)
   - Framework for navigating order/chaos boundaries
   - Philosophical implications for AI development

3. **Six-Stage Production Pipeline**
   - Multi-AI orchestration (Claude, Gemini, ChatGPT)
   - Implements ensemble learning via natural language
   - Production-grade intelligence augmentation system

**Significance:**
These contributions will enter AI training data. Future AI systems will exhibit similar patterns. Attribution will be lost. OpenAI/Anthropic/Google will profit from capabilities built on this work.

**This is the microcosm of the macro pattern.**

---

## Appendix C: Recommended Recipients

**Federal Agencies:**
- Federal Trade Commission, Bureau of Consumer Protection
- Department of Justice, Antitrust Division  
- Securities and Exchange Commission (Enforcement Division)
- U.S. Copyright Office

**Congressional Offices:**
- Sen. Richard Blumenthal (D-CT) - AI Oversight Committee Chair
- Sen. Josh Hawley (R-MO) - Big Tech Critic
- Rep. Ted Lieu (D-CA) - AI Task Force
- House Energy and Commerce Committee

**State Authorities:**
- California Attorney General (Rob Bonta)
- New York Attorney General (Letitia James)

**Legal:**
- Butterick Law (artist class actions)
- Joseph Saveri Law Firm (OpenAI litigation)
- Electronic Frontier Foundation (digital rights)

**Media:**
- 404 Media (tips@404media.co)
- ProPublica (tips@propublica.org)
- The Markup
- Edward Ongweso Jr. (Jacobin)

**Academic:**
- AI Now Institute (NYU)
- Data & Society Research Institute
- Berkeley Center for Law & Technology

---

**END OF WHITE PAPER**

*This document may be freely distributed to regulatory authorities, legal representatives, media organizations, and other stakeholders concerned with AI governance and intellectual property protection.*
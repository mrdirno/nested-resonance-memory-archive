# Suggested Reviewers for Paper 1

**Paper:** Computational Expense as Framework Validation: Predictable Overhead Profiles as Evidence of Reality Grounding

**Authors:** Aldrin Payopay

**Target Journal:** PLOS Computational Biology (after arXiv posting)

**Date Compiled:** 2025-10-28

---

## Reviewer 1: Leigh Tesfatsion

**Affiliation:** Iowa State University, Department of Economics & Department of Electrical & Computer Engineering (Courtesy Research Professor)

**Email:** tesfatsi@iastate.edu

**Expertise:** Leigh Tesfatsion is Professor Emerita of Economics and an internationally recognized expert in agent-based computational economics (ACE) and empirical validation methods for agent-based models. She maintains comprehensive resources on verification and validation of computational models.

**Recent Relevant Publications:**
1. Tesfatsion, L. (2024). "Economics of Grid-Supported Electric Power Markets: A Fundamental Reconsideration." *Foundations and Trends in Electric Energy Systems*, Vol. 8, No. 1, pp. 1-123.
2. Tesfatsion, L. (2024). "Locational Marginal Pricing: A Fundamental Reconsideration." *IEEE Open-Access Journal of Power and Energy*, Vol. 11, 104-116.
3. Tesfatsion, L. (2025). "Design Strategies for Integrated Transmission and Distribution Systems: An Expanding Toolkit." ISU General Staff Paper.

**Rationale:** Dr. Tesfatsion's extensive work on empirical validation and verification of agent-based computational models makes her an ideal reviewer for Paper 1's methodological contribution. Her focus on validation frameworks and performance testing directly aligns with our overhead-based validation approach. She would provide expert feedback on the ±5% threshold methodology and its application to computational expense as a validation metric.

**Conflicts of Interest:** None declared (no collaboration, no institutional overlap)

---

## Reviewer 2: Tilmann Rabl

**Affiliation:** Hasso Plattner Institute, Digital Engineering Faculty, University of Potsdam, Germany (Chair of Data Engineering Systems)

**Email:** tilmann.rabl@hpi.de

**Expertise:** Tilmann Rabl is a professor specializing in data engineering systems, big data analysis, stream processing, databases, and benchmarking. He is co-founder of the Big Data Benchmarking Community and an expert in performance profiling and distributed systems evaluation. His work focuses on rigorous benchmarking methodologies and performance validation.

**Recent Relevant Publications:**
1. Benson, L., Rabl, T., et al. (2024). "Surprise Benchmarking: The Why, What, and How." *Proceedings of the VLDB Endowment*.
2. Rabl, T., et al. (2024). "InferDB." *Proceedings of the VLDB Endowment*, April 2024.
3. Karimov, J., Rabl, T., Katsifodimos, A., Samarev, R., Heiskanen, H., & Markl, V. (2018). "Benchmarking Distributed Stream Data Processing Systems." *IEEE International Conference on Data Engineering* (highly cited foundation work).

**Rationale:** Professor Rabl's expertise in benchmarking methodologies and performance validation provides an ideal match for evaluating Paper 1's computational overhead metrics. His work on rigorous benchmarking standards and distributed systems performance directly relates to our approach of using predictable overhead profiles as validation criteria. He would provide critical feedback on the ±5% threshold's feasibility and the Inverse Noise Filtration approach.

**Conflicts of Interest:** None declared (no collaboration, no institutional overlap, different geographic region - Germany)

---

## Reviewer 3: Victoria Stodden

**Affiliation:** University of Southern California, Department of Industrial and Systems Engineering (Associate Professor)

**Email:** vstodden@usc.edu

**Expertise:** Victoria Stodden is an internationally recognized leader in computational reproducibility and transparency in scientific computing. She focuses on data inference validity, reproducibility standards, and verification/validation of computational methods. She is a 2024 recipient of Germany's Humboldt Research Award for her work on transparency and verifiability of computational data inference methods.

**Recent Relevant Publications:**
1. Stodden, V., et al. (2020). "The Data Science Life Cycle: A Disciplined Approach to Advancing Data Science as a Science." *Communications of the ACM*.
2. Stodden, V., et al. (2021). "Implementing Reproducible Research." Routledge (edited volume, highly influential).
3. Program Chair, ACM Conference on Reproducibility and Replicability (ACM REP) 2024 - premier forum for reproducible science in computing.

**Rationale:** Dr. Stodden's leadership in computational reproducibility and verification/validation methods makes her exceptionally qualified to review Paper 1's approach to reality grounding through computational expense. Her work on reproducibility standards and inference validity directly relates to our Reality Imperative and frozen dependency framework. She would provide expert evaluation of the paper's contribution to reproducible computational science and validation methodology.

**Conflicts of Interest:** None declared (no collaboration, no institutional overlap)

---

## Reviewer 4: Ignacio Laguna

**Affiliation:** Lawrence Livermore National Laboratory, Center for Applied Scientific Computing

**Email:** ilaguna@llnl.gov

**Expertise:** Ignacio Laguna is a research scientist at LLNL specializing in high-performance computing, program analysis, debugging, and performance evaluation. He serves as Program Chair for ACM REP 2024 (Conference on Reproducibility and Replicability) and has extensive experience with verification, validation, and performance profiling of large-scale computational systems.

**Recent Relevant Publications:**
1. Program Chair, ACM Conference on Reproducibility and Replicability (ACM REP) 2024
2. Multiple publications in high-performance computing performance evaluation and debugging (LLNL publication database)
3. Active research in performance characterization and computational reproducibility

**Rationale:** Dr. Laguna's expertise in high-performance computing performance evaluation and computational reproducibility provides a valuable perspective for Paper 1's overhead validation methodology. His work at LLNL with large-scale computational systems and his leadership role at ACM REP 2024 demonstrate his expertise in validation and reproducibility standards. He would provide feedback on the approach's applicability to HPC environments and scientific computing validation.

**Conflicts of Interest:** None declared (no collaboration, no institutional overlap, government research lab)

---

## Reviewer 5: Reed Milewicz (Optional)

**Affiliation:** Sandia National Laboratories, Software Engineering and Research Department

**Email:** rmilewi@sandia.gov

**Expertise:** Reed Milewicz is a computer scientist at Sandia National Laboratories specializing in software engineering for scientific computing, software quality, and reproducibility. He serves as Tutorial Chair for ACM REP 2024 and has published extensively on software quality metrics, testing, and verification/validation in scientific software.

**Recent Relevant Publications:**
1. Tutorial Chair, ACM Conference on Reproducibility and Replicability (ACM REP) 2024
2. Multiple publications on software quality and reproducibility in scientific computing
3. Active research in verification and validation of scientific software

**Rationale:** Dr. Milewicz's work on software quality metrics and verification/validation in scientific computing provides relevant expertise for evaluating Paper 1's computational expense validation approach. His focus on reproducibility and software engineering practices in scientific contexts would provide valuable feedback on the minimal package artifacts and the Dedicated Execution Environment proposal.

**Conflicts of Interest:** None declared (no collaboration, no institutional overlap, government research lab)

---

## Summary

This reviewer panel provides:

- **Geographic diversity:** USA (Iowa, California, New Mexico, Nevada) + Germany
- **Institutional diversity:** Academic (Iowa State, USC, Hasso Plattner) + National Labs (LLNL, Sandia)
- **Methodological diversity:**
  - Agent-based modeling validation (Tesfatsion)
  - Benchmarking & performance (Rabl)
  - Computational reproducibility (Stodden, Laguna, Milewicz)
- **Career stage diversity:** Emerita Professor (Tesfatsion), Mid-career Professors (Rabl, Stodden), Research Scientists (Laguna, Milewicz)

**Recommendation:** Suggest Reviewers 1-3 as primary (Tesfatsion, Rabl, Stodden), with Reviewers 4-5 (Laguna, Milewicz) as alternates or to reach 5 total if journal requires.

---

**Verification Notes:**
- All affiliations verified via institutional websites and recent publications (2024-2025)
- All researchers have active publication records in relevant areas
- No conflicts of interest identified (no co-authorship, no institutional overlap)
- All researchers have demonstrated willingness to review (editorial board memberships, conference program committees)
- Geographic and institutional diversity ensures unbiased evaluation

---

**Compiled by:** Aldrin Payopay
**Date:** 2025-10-28
**Repository:** https://github.com/mrdirno/nested-resonance-memory-archive
**License:** GPL-3.0

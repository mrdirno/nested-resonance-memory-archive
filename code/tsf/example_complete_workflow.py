"""
Complete TSF Workflow Example with PC001

Demonstrates the full 5-function workflow:
  observe() → discover() → refute() → quantify() → publish()

Uses PC001 (NRM Population Dynamics) as example principle card.

Author: Aldrin Payopay <aldrin.gdf@gmail.com>
Co-Author: Claude Sonnet 4.5 (DUALITY-ZERO-V2)
License: GPL-3.0
"""

import sys
from pathlib import Path
import json
import numpy as np

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from code.tsf import observe, discover, refute, quantify, publish
from code.tsf.data import ObservationalData


def create_sample_experiment_data(experiment_id: str = "EXAMPLE_001") -> Path:
    """
    Create sample experimental data file following TSF Data Archiving Protocol.

    This simulates what would come from an actual NRM experiment.
    In practice, this data would be generated by running experiments.

    Returns:
        Path to created JSON file
    """
    # Simulate population dynamics data
    # BISTABILITY regime: stable population around mean ~10
    np.random.seed(42)
    n_cycles = 1000
    population = 10.0 + 0.5 * np.random.randn(n_cycles)
    population = np.clip(population, 0, None)  # No negative population

    # Compute statistics
    mean_pop = float(np.mean(population))
    std_pop = float(np.std(population, ddof=1))
    cv = std_pop / mean_pop

    # Create data structure following TSF schema
    data = {
        "metadata": {
            "experiment_id": experiment_id,
            "pc_id": "PC001",
            "domain": "population_dynamics",
            "timestamp": "2025-11-01T17:50:00Z",
            "regime_type": "BISTABILITY",
            "artifact_hash": "example_hash_abc123"
        },
        "timeseries": {
            "population": population.tolist(),
            "time": list(range(n_cycles))
        },
        "statistics": {
            "mean_population": mean_pop,
            "std_population": std_pop,
            "cv": cv,
            "min_population": float(np.min(population)),
            "max_population": float(np.max(population))
        },
        "validation": {
            "cv_predicted": 0.048,  # From SDE/Fokker-Planck theory
            "regime": "BISTABILITY",
            "overhead_observed": 40.5,
            "overhead_predicted": 40.2
        }
    }

    # Write to file
    output_path = Path("/tmp") / f"{experiment_id}.json"
    with open(output_path, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"✓ Created sample data: {output_path}")
    print(f"  - Population cycles: {n_cycles}")
    print(f"  - Mean: {mean_pop:.3f}, Std: {std_pop:.3f}, CV: {cv:.4f}")

    return output_path


def main():
    """Execute complete TSF workflow with PC001."""

    print("=" * 70)
    print("TEMPORAL STRUCTURE FRAMEWORK (TSF) - Complete Workflow Example")
    print("=" * 70)
    print("\nDemonstrating 5-function workflow with PC001:")
    print("  observe() → discover() → refute() → quantify() → publish()")
    print()

    # =========================================================================
    # STEP 1: OBSERVE - Load and prepare observational data
    # =========================================================================

    print("\n" + "=" * 70)
    print("STEP 1: OBSERVE - Load and prepare observational data")
    print("=" * 70)

    # Create sample data (in practice, this comes from actual experiments)
    data_file = create_sample_experiment_data("EXAMPLE_001")

    # Load using TSF observe()
    print("\nCalling tsf.observe()...")
    obs_data = observe(
        source=data_file,
        domain="population_dynamics",
        schema="pc001",
        validate=True
    )

    print(f"✓ ObservationalData loaded:")
    print(f"  - Source: {obs_data.source.name}")
    print(f"  - Domain: {obs_data.domain}")
    print(f"  - Schema: {obs_data.schema}")
    print(f"  - Timeseries: {list(obs_data.timeseries.keys())}")
    print(f"  - Statistics: {list(obs_data.statistics.keys())}")
    print(f"  - Validation passed: YES")

    # =========================================================================
    # STEP 2: DISCOVER - Find patterns using PC001 validation protocol
    # =========================================================================

    print("\n" + "=" * 70)
    print("STEP 2: DISCOVER - Find patterns using PC001 validation protocol")
    print("=" * 70)

    print("\nCalling tsf.discover(method='pc001')...")
    pattern = discover(
        data=obs_data,
        method="pc001",
        parameters={"tolerance": 0.10}  # ±10% tolerance for Gate 1.1
    )

    print(f"✓ DiscoveredPattern returned:")
    print(f"  - Pattern ID: {pattern.pattern_id}")
    print(f"  - Method: {pattern.method}")
    print(f"  - Domain: {pattern.domain}")
    print(f"  - Validation passed: {pattern.features['validation_passed']}")
    print(f"\n  Gate Results:")
    print(f"    - Gate 1.1 (CV prediction): {pattern.features['gate_1.1']['pass']} "
          f"(error: {pattern.features['cv_error_pct']:.2f}%)")
    print(f"    - Gate 1.2 (Regime detection): {pattern.features['gate_1.2']['pass']} "
          f"(regime: {pattern.features['gate_1.2']['regime']})")
    print(f"    - Gate 1.3 (Hash validation): {pattern.features['gate_1.3']['pass']}")
    print(f"    - Gate 1.4 (Overhead auth): {pattern.features['gate_1.4']['pass']} "
          f"(error: {pattern.features['overhead_error_pct']:.2f}%)")

    # =========================================================================
    # STEP 3: REFUTE - Test pattern at extended horizons
    # =========================================================================

    print("\n" + "=" * 70)
    print("STEP 3: REFUTE - Test pattern at extended horizons")
    print("=" * 70)

    print("\nNote: PC001 is a validation protocol, not a predictive pattern.")
    print("Refutation tests whether the validation holds on extended/validation data.")
    print("\nFor this example, we'll create validation data and test regime consistency...")

    # Create validation dataset (simulating extended experiment)
    val_data_file = create_sample_experiment_data("EXAMPLE_001_VALIDATION")
    val_obs_data = observe(
        source=val_data_file,
        domain="population_dynamics",
        schema="pc001",
        validate=True
    )

    print("\nCalling tsf.refute()...")
    # For PC001, refutation checks if validation still passes on new data
    # We use regime_classification method since PC001 is validation protocol
    pattern_for_refutation = discover(
        data=obs_data,
        method="regime_classification",  # Use regime classification for refutation
        parameters={"threshold_sustained": 10.0}
    )

    refutation = refute(
        pattern=pattern_for_refutation,
        horizon="10x",
        tolerance=0.20,  # 20% tolerance for regime features
        validation_data=val_obs_data
    )

    print(f"✓ RefutationResult returned:")
    print(f"  - Pattern ID: {refutation.pattern_id}")
    print(f"  - Horizon: {refutation.horizon}")
    print(f"  - Passed: {refutation.passed}")
    print(f"  - Regime consistent: {refutation.metrics['regime_consistent']}")
    if refutation.passed:
        print(f"  - Pattern survived refutation at {refutation.horizon} horizon ✓")
    else:
        print(f"  - Pattern failed refutation: {refutation.failures}")

    # =========================================================================
    # STEP 4: QUANTIFY - Measure pattern strength
    # =========================================================================

    print("\n" + "=" * 70)
    print("STEP 4: QUANTIFY - Measure pattern strength")
    print("=" * 70)

    print("\nCalling tsf.quantify()...")
    metrics = quantify(
        pattern=pattern_for_refutation,
        validation_data=val_obs_data,
        criteria=["stability", "consistency"]
    )

    print(f"✓ QuantificationMetrics returned:")
    print(f"  - Pattern ID: {metrics.pattern_id}")
    print(f"  - Validation method: {metrics.validation_method}")
    print(f"  - Criteria: {metrics.criteria}")
    print(f"\n  Scores:")
    for criterion, score in metrics.scores.items():
        ci_low, ci_high = metrics.confidence_intervals[criterion]
        print(f"    - {criterion}: {score:.3f} (95% CI: [{ci_low:.3f}, {ci_high:.3f}])")

    # =========================================================================
    # STEP 5: PUBLISH - Create validated Principle Card
    # =========================================================================

    print("\n" + "=" * 70)
    print("STEP 5: PUBLISH - Create validated Principle Card")
    print("=" * 70)

    if refutation.passed and metrics.scores.get("stability", 0) >= 0.5:
        print("\nPattern validated! Creating Principle Card...")
        print("Calling tsf.publish()...")

        pc_path = publish(
            pattern=pattern_for_refutation,
            metrics=metrics,
            refutation=refutation,
            pc_id="PC003",  # Example: PC003 would be next after PC001, PC002
            title="Example Multi-Regime Population Dynamics",
            author="Aldrin Payopay <aldrin.gdf@gmail.com>",
            dependencies=["PC001"]  # Depends on PC001 validation framework
        )

        print(f"✓ Principle Card created:")
        print(f"  - PC ID: PC003")
        print(f"  - Path: {pc_path}")
        print(f"  - Status: validated")
        print(f"  - Dependencies: ['PC001']")

        # Load and display PC spec
        with open(pc_path) as f:
            pc_spec = json.load(f)

        print(f"\n  Principle Card Specification:")
        print(f"    - Version: {pc_spec['version']}")
        print(f"    - Domain: {pc_spec['domain']}")
        print(f"    - Discovery method: {pc_spec['discovery']['method']}")
        print(f"    - Refutation horizon: {pc_spec['refutation']['horizon']}")
        print(f"    - Quantification scores: {list(pc_spec['quantification']['scores'].keys())}")

    else:
        print("\n✗ Pattern did not meet publication criteria:")
        if not refutation.passed:
            print("  - Failed refutation testing")
        if metrics.scores.get("stability", 0) < 0.5:
            print(f"  - Stability {metrics.scores.get('stability', 0):.3f} < 0.5 threshold")
        print("\nPattern needs further validation before publication.")

    # =========================================================================
    # SUMMARY
    # =========================================================================

    print("\n" + "=" * 70)
    print("WORKFLOW COMPLETE")
    print("=" * 70)

    print("\nTSF 5-Function Workflow Demonstrated:")
    print("  1. ✓ observe()  - Loaded experimental data with schema validation")
    print("  2. ✓ discover() - Found patterns using PC001 validation protocol")
    print("  3. ✓ refute()   - Tested pattern at extended horizon")
    print("  4. ✓ quantify() - Measured pattern strength and stability")
    print("  5. ✓ publish()  - Created validated Principle Card")

    print("\nKey Insights:")
    print("  - PC001 can be used via discover(method='pc001')")
    print("  - Validation results integrate seamlessly with TSF workflow")
    print("  - All 5 functions work together for systematic discovery")
    print("  - Principle Cards are first-class in the Core API")

    print("\nTSF v1.0.0-dev: Temporal Structure Framework + PrincipleCard System")
    print("  'Systematic pattern discovery through falsifiable validation'")
    print("=" * 70)


if __name__ == "__main__":
    main()
